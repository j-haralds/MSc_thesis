\section{Overview of Batteries}
\begin{itemize}
    \item A battery's parts, anode, cathode, electrolyte. 
    \begin{itemize}
        \item Anode: Primarily graphite. Light-weight, high conductivity
        \item Cathode: \gls{li}-oxides commonly \gls{nmc} or \gls{nca}. 
        \item Electrolyte: Up to this point mainly liquid electrolytes, however, solid-state electrolyte are in development. 
        \item Lithium: Why use \gls{li} in the first place? (Fun fact: \gls{li} was first discovered on Ut√∂ in the Stockholm archipelago) \gls{li}, the third lightest element on earth, has the among the lowest density out of all solids and it has a low ionization energy. With its low mass it uses less energy as it diffuses through the cell.  
    \end{itemize}
    \item The general working principle: charging and discharging. Potential negative effects that need to be considered. Heat development, or other factors affecting performance of the cell. Intercalation of $\LIP$ 
    \item What equations are governing? (Thermodynamics, Mechanics, electric, chemical) 
    \item The different scales: cell vs.\ module vs.\ pack. What can be inferred from the different scales? How does microscopic properties influence macroscopic ones and vice versa. 
    \item 
\end{itemize}

\section{Neural Networks} 

\begin{itemize}
    \item General overview of neural networks. Explain neurons, weights, bias, loss function, different types of NNs. RNN?? 
    \item Motivate the use of \glspl{nn} w/ the Universal Approximation Theorem, i.e., the fact that a single layer perceptron \gls{nn} can achieve universallity. Put differently, any continuous function can be modelled to the desired degree of accuracy with a \gls{nn} (\href{https://en.wikipedia.org/wiki/Universal_approximation_theorem}{wiki-link}). 
\end{itemize}

\section{Physics Informed Neural Networks}
\begin{itemize}
    \item Generally describe the setup we're dealing with \eref{eq:pde} to \ref{eq:bound}. 
    \item How to make NNs PINNs? Regularization term in the loss function that penalizes NN solutions that aren't physically viable. 
\end{itemize}

\begin{align}
f\!\left(
\bm{x}, t, \frac{\partial u}{\partial \bm{x}}, \frac{\partial u}{\partial t}, \bm{\lambda}
\right)
&= 0,
\quad \bm{x} \in \Omega,\; t \in [0,\,\tau],
\label{eq:pde} \\[0.5em]
u(\bm{x},0)
&= h(\bm{x}),
\quad \bm{x} \in \Omega,
\label{eq:init} \\[0.5em]
u(\bm{x},t)
&= g(\bm{x},t),
\quad \bm{x} \in \partial\Omega,\; t \in [0,\,\tau].
\label{eq:bound}
\end{align}

\begin{figure}[H]
\centering
    \includegraphics[width=0.72\textwidth]{figs/NN.pdf}
    \caption{Schematic view of a neural network where each circle represents a single neuron. An input signal $\bm{X} = \{\bm{x}_i\}_{i=0}^{N}$ enters the model in the input layer, proceeds through the hidden layers, and exits from the output layer. This process produces an output $\bm{Y} = \{\bm{y}_i\}_{i=0}^{M}$, which can be interpreted as a prediction based on the input data.}
    \label{NeuNet}
\end{figure}


Loss function, such as \gls{rmse}.