\section*{Literture from RISE}
\begin{itemize}
    \item \textbf{Matty 2025 MSc thesis Chalmers} - Modelling of Electrode Swelling in Lithium ion Batteries.pdf. Thesis work from 2025 at VCC on multiscale modelling of cyclic swelling of batteries. This thesis introduces the topic of electro-chemo-mechanical modelling of batteries, and the current thesis will explore if/how AI/ML-methods can be used for upscaling the response, going from the corresponding micro/meso- to macroscale.
    \item \textbf{Asheri 2023} - Data-driven multiscale simulation of solid-state batteries via machine learning.pdf. Paper from a group in Darmstadt, Germany (work done with VW), where they train a surrogate model (based on NN) on a large set of simulation data, which is afterwards employed to predict the cell performance (for solid-state batteries, which in general or from a modelling perspective, is very similar to conventional batteries).
    \item \textbf{Meyer and Ekre 2023} – Thermodynamically consistent neural network plasticity modelling and discovery of evolution laws. Very nice paper from Knut and Fredrik (another previous, Chalmers-Fredrik) on using thermodynamically consistent NN for plasticity modelling and discovery of evolution laws.
    \item \textbf{Panahi et al 2025} - Fast and generalisable parameter-embedded neural operators for lithium-ion battery simulation.pdf. Recent paper from RWTH Aachen+Imperial on machine learning surrogates for battery modelling.
    \item \textbf{Guo 2025} - Uncovering the impact of battery design parameters on health and lifetime using short charging segments.pdf. Recent paper from Uppsala+Aalborg on ML framework for corresponding problem.
    \item \textbf{Schmid 2024} - Sequential Multi-Scale Modeling Using an Artificial Neural Network-Based Surrogate Material Model for Predicting the Mechanical Behavior of a Li-Ion Pouch Cell Under Abuse Conditions.pdf. Long, but relatively self-explanatory, title. Basically, this paper uses a similar approach, but for the pure mechanical response of batteries under mechanical impact loading (or what is referred to as abuse conditions).
\end{itemize}
\section{Overview of Batteries}
\begin{itemize}
    \item A battery's parts, anode, cathode, electrolyte. 
    \begin{itemize}
        \item Anode: Primarily graphite. Light-weight, high conductivity
        \item Cathode: \gls{li}-oxides commonly \gls{nmc} or \gls{nca}. 
        \item Electrolyte: Up to this point mainly liquid electrolytes, however, solid-state electrolyte are in development. 
        \item Lithium: Why use \gls{li} in the first place? (Fun fact: \gls{li} was first discovered on Utö in the Stockholm archipelago) \gls{li}, the third lightest element, has the lowest density out of all solids and it also has a low ionization energy. With its low mass it uses less energy as it diffuses through the cell.  
    \end{itemize}
    \item The general working principle: charging and discharging. Potential negative effects that need to be considered. Heat development, or other factors affecting performance of the cell. Intercalation of $\LIP$ $\mathrm{Li}\rightarrow\LIP+e^-$
    \item What equations are governing? (Thermodynamics, Mechanics, electric, chemical). The \gls{dfn} seems to be the gold-standard when it comes to modeling (\href{https://docs.pybamm.org/en/stable/source/examples/notebooks/models/DFN.html}{DFN}). 
    \item The different scales: cell vs.\ module vs.\ pack. What can be inferred from the different scales? How does microscopic properties influence macroscopic ones and vice versa. 
\end{itemize}

\section{Neural Networks} 

\begin{itemize}
    \item General overview of neural networks. Explain neurons, weights, bias, loss function, different types of NNs. RNN?? 
    \item Motivate the use of \glspl{nn} w/ the Universal Approximation Theorem, i.e., the fact that a single layer perceptron \gls{nn} can achieve universallity. Put differently, any continuous function can be modelled to the desired degree of accuracy with a \gls{nn} ({\href{https://en.wikipedia.org/wiki/Universal_approximation_theorem}{wiki-link}}). 
\end{itemize}

\section{Physics Informed Neural Networks}
\begin{itemize}
    \item Generally describe the setup we're dealing with \eref{eq:pde} to \ref{eq:bound}. \eref{eq:pde} describes the \gls{pde} residual, \eref{eq:init} the \gls{ic} and \eref{eq:bound} the \gls{bc} \cite{Luo_et}.
    \item How to make NNs PINNs? Regularization term in the loss function that penalizes NN solutions that aren't physically viable. 
\end{itemize}

\begin{align}
f\!\left(
\bm{x}, t, \frac{\partial u}{\partial \bm{x}}, \frac{\partial u}{\partial t}, \bm{\lambda}
\right)
&= 0,
\quad \bm{x} \in \Omega,\; t \in [0,\,\tau],
\label{eq:pde} \\[0.5em]
u(\bm{x},0)
&= h(\bm{x}),
\quad \bm{x} \in \Omega,

\label{eq:init} \\[0.5em]
u(\bm{x},t)
&= g(\bm{x},t),
\quad \bm{x} \in \partial\Omega,\; t \in [0,\,\tau].
\label{eq:bound}
\end{align}

\begin{figure}[H]
\centering
    \includegraphics[width=0.72\textwidth]{figs/NN.pdf}
    \caption{Schematic view of a neural network where each circle represents a single neuron. An input signal $\bm{X} = \{\bm{x}_i\}_{i=0}^{N}$ enters the model in the input layer, proceeds through the hidden layers, and exits from the output layer. This process produces an output $\bm{Y} = \{\bm{y}_i\}_{i=0}^{M}$, which can be interpreted as a prediction based on the input data.}
    \label{NeuNet}
\end{figure}

\subsection{Pipeline for the Training Procedure}
Following the suggested pipeline from Wang \etal, the initial step is to ensure inputs and outputs are within the same range through non-dimensionalization of the \gls{pde} system \cite{Expert}. Unevenly distributed values might: negatively influence the training process leading to unstable convergence, prevent the model from finding important correlations, it could also prevent vanishing gradient phenomenon through a more balanced initialization scheme. In practice, non-dimensionalization is done by reducing the system to a dimensionless version by first choosing some fundamental units or characteristics, and then scaling the remaining parameters such that all terms are dimensionless and of first order. \textcolor{cyan}{An example of non-dimensionalization for Navier-Stoke} $$\frac{\partial\bm{u}}{\partial t} + (\bm{u}\cdot\vec{\nabla})\bm{u}=-\frac{1}{\rho}\vec{\nabla}p+\nu\nabla^2\bm{u}\longmapsto
\frac{\partial\bm{u}^*}{\partial t^*} + (\bm{u}^*\cdot\vec{\nabla}^*)\bm{u}^*=-\vec{\nabla}^*p^*+\frac{1}{\mathrm{Re}}{\nabla^*}^2\bm{u}^*,$$ where $\bm{u}^*=\bm{u}/U$, $\vec{\nabla}^*=L\vec{\nabla}$, $t^*=tU/L$, and $p^*=p/(\rho U^2)$

The subsequent step is to design the \gls{nn} architecture that could represent the \gls{pde}. Wang \etal\ recommends using either \gls{ffe} or \gls{rwf} to accelerate the convergence \cite{Expert}. Lastly, the training procedure should contain loss balancing, casual training and/or curriculum training. 